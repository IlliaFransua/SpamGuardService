{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jtUWsqYI1Gtb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for file in ['model.pkl', 'spam_data.npz', 'vectorizer.pkl']:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"{file} removed\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDetectionModel:\n",
    "    MODEL_FILE = \"spam_model.pkl\"\n",
    "    DATA_FILE = \"spam_data.npz\"\n",
    "    VECTOR_FILE = \"vectorizer.pkl\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.check_and_create_files()\n",
    "\n",
    "        self.vectorizer = joblib.load(self.VECTOR_FILE)\n",
    "        self.classifier = joblib.load(self.MODEL_FILE)\n",
    "        self.load_training_data()\n",
    "\n",
    "    def check_and_create_files(self):\n",
    "        if not os.path.exists(self.VECTOR_FILE):\n",
    "            # print(f\"File {self.VECTOR_FILE} not found. Creating new\")\n",
    "            joblib.dump(CountVectorizer(), self.VECTOR_FILE)\n",
    "\n",
    "        if not os.path.exists(self.MODEL_FILE):\n",
    "            # print(f\"File {self.MODEL_FILE} not found. Creating new\")\n",
    "            joblib.dump(SGDClassifier(loss='log_loss'), self.MODEL_FILE)\n",
    "\n",
    "        if not os.path.exists(self.DATA_FILE):\n",
    "            # print(f\"File {self.DATA_FILE} not found. Creating new\")\n",
    "            self.save_training_data()\n",
    "\n",
    "    def load_training_data(self):\n",
    "        if os.path.exists(self.DATA_FILE) and os.path.getsize(self.DATA_FILE) > 0:\n",
    "            data = np.load(self.DATA_FILE, allow_pickle=True)\n",
    "            self.texts = data['texts'].tolist()\n",
    "            self.labels = data['labels'].tolist()\n",
    "        else:\n",
    "            self.texts = []\n",
    "            self.labels = []\n",
    "\n",
    "    def save_training_data(self):\n",
    "        np.savez(self.DATA_FILE, texts=self.texts, labels=self.labels)\n",
    "\n",
    "    def train(self, spam_texts: List[str], non_spam_texts: List[str]):\n",
    "        new_texts = spam_texts + non_spam_texts\n",
    "        new_labels = [1] * len(spam_texts) + [0] * len(non_spam_texts)\n",
    "\n",
    "        if not new_texts:\n",
    "            raise ValueError(\"Training data cannot be empty\")\n",
    "\n",
    "        self.texts.extend(new_texts)\n",
    "        self.labels.extend(new_labels)\n",
    "        self.save_training_data()\n",
    "\n",
    "        X_all = self.vectorizer.fit_transform(self.texts)\n",
    "        y_all = np.array(self.labels)\n",
    "        joblib.dump(self.vectorizer, self.VECTOR_FILE)\n",
    "\n",
    "        self.classifier.partial_fit(X_all, y_all, classes=np.array([0, 1]))\n",
    "        joblib.dump(self.classifier, self.MODEL_FILE)\n",
    "\n",
    "    def predict(self, text: str) -> bool:\n",
    "        X = self.vectorizer.transform([text])\n",
    "        return self.classifier.predict(X)[0]\n",
    "\n",
    "    def test(self, spam_texts: List[str], non_spam_texts: List[str]) -> float:\n",
    "        texts = spam_texts + non_spam_texts\n",
    "        labels = [1] * len(spam_texts) + [0] * len(non_spam_texts)\n",
    "\n",
    "        if not texts:\n",
    "            raise ValueError(\"Test data cannot be empty\")\n",
    "\n",
    "        X = self.vectorizer.transform(texts)\n",
    "        return self.classifier.score(X, labels)\n",
    "\n",
    "    def save_all(self):\n",
    "        joblib.dump(self.vectorizer, self.VECTOR_FILE)\n",
    "        joblib.dump(self.classifier, self.MODEL_FILE)\n",
    "        self.save_training_data()"
   ],
   "metadata": {
    "id": "-RUCMzpm1KSX"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv('/content/drive/MyDrive/spam_text_train_dataset.csv')\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/spam_text_test_dataset.csv')\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"text\"])\n",
    "test_df = test_df.dropna(subset=[\"text\"])\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 1000\n",
    "\n",
    "spam_train_texts = train_df[train_df[\"label\"] == 1][\"text\"].tolist()\n",
    "non_spam_train_texts = train_df[train_df[\"label\"] == 0][\"text\"].tolist()\n",
    "\n",
    "spam_test_texts = test_df[test_df[\"label\"] == 1][\"text\"].tolist()\n",
    "non_spam_test_texts = test_df[test_df[\"label\"] == 0][\"text\"].tolist()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CblVbhSy1Ln6",
    "outputId": "1124a58a-dfda-41f4-859c-96134fd8470b"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "97565\n",
      "24396\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "spam_detector = SpamDetectionModel()\n",
    "\n",
    "all_train_texts = spam_train_texts + non_spam_train_texts\n",
    "\n",
    "X_all = spam_detector.vectorizer.fit_transform(all_train_texts)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for i in range(0, len(spam_train_texts), batch_size):\n",
    "      spam_batch = spam_train_texts[i:i+batch_size]\n",
    "      non_spam_batch = non_spam_train_texts[i:i+batch_size]\n",
    "\n",
    "      X_batch = spam_detector.vectorizer.transform(spam_batch + non_spam_batch)\n",
    "\n",
    "      y_batch = [1] * len(spam_batch) + [0] * len(non_spam_batch)\n",
    "      spam_detector.classifier.partial_fit(X_batch, y_batch, classes=np.array([0, 1]))\n",
    "\n",
    "spam_detector.save_all()\n",
    "\n",
    "print(\"Model is trained\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zuz4X6c1NGi",
    "outputId": "829b020e-3051-4832-cfa5-fcfd35a5bc6e"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model is trained\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_test = spam_detector.vectorizer.transform(spam_test_texts + non_spam_test_texts)\n",
    "y_test = [1] * len(spam_test_texts) + [0] * len(non_spam_test_texts)\n",
    "accuracy = spam_detector.classifier.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXgKgerz1Oxb",
    "outputId": "12ef0cde-6403-40d9-d293-5d5469c745a7"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 89.45%\n"
     ]
    }
   ]
  }
 ]
}
